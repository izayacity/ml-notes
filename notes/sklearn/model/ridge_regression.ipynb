{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a7381235-2778-4d69-8a25-199889214c40",
   "metadata": {},
   "source": [
    "While ordinary least squares regression is a good way to fit a linear model onto a dataset, it relies on the fact that the dataset's features are each independent, i.e. uncorrelated. When many of the dataset features are linearly correlated, e.g. if a dataset has multiple features depicting the same price in different currencies, it makes the least squares regression model highly sensitive to noise in the data.\n",
    "\n",
    "Because real life data tends to have noise, and will often have some linearly correlated features in the dataset, we combat this by performing regularization. For ordinary least squares regression, the goal is to find the weights (coefficients) for the linear model that minimize the sum of squared residuals:\n",
    "\n",
    "​i=1\n",
    "​∑\n",
    "​n\n",
    "​​ (x\n",
    "​​ ⋅w−y)\n",
    "​2\n",
    "​​\n",
    "\n",
    "where each xi represents a data observation and yi is the corresponding label.\n",
    "\n",
    "For regularization, the goal is to not only minimize the sum of squared residuals, but to do this with coefficients as small as possible. The smaller the coefficients, the less susceptible they are to random noise in the data. The most commonly used form of regularization is ridge regularization.\n",
    "\n",
    "With ridge regularization, the goal is now to find the weights that minimize the following quantity:\n",
    "\n",
    "α(∣∣w∣∣\n",
    "​_2)\n",
    "​^2\n",
    "​​\n",
    "​​ +\n",
    "​(i=1\n",
    "​∑\n",
    "​n\n",
    "​​ (x\n",
    "​i\n",
    "​​ ⋅w−y\n",
    "​i\n",
    "​​ )\n",
    "​2)\n",
    "​​\n",
    "\n",
    "where α is a non-negative real number hyperparameter and ||w||_2 represents the L2 norm of the weights. The additional \n",
    "\n",
    "α(∣∣w∣∣\n",
    "​_2)\n",
    "​^2\n",
    "\n",
    "is referred to as the penalty term, since it penalizes larger weight values. Larger quantities of α will put greater emphasis on the penalty term, forcing the model to have even smaller weight values.\n",
    "\n",
    "![title](img/ridge_compare.png)\n",
    "\n",
    "The plot above shows an example of ordinary least squares regression models vs. ridge regression models. The two red crosses mark the points (0.5, 0.5) and (1, 1), and the blue lines are the regression lines for those two points. Each of the grey lines are the regression lines for the original points with added noise (which is signified by the grey points).\n",
    "\n",
    "The ordinary least squares regression is much more susceptible to being influenced by the added noise, as there is a much larger degree of variance in the grey regression lines compared to the ridge regression.\n",
    "\n",
    "## Choosing the best alpha\n",
    "\n",
    "In scikit-learn, we implement ridge regression in essentially the same way we implement ordinary least squares regression. We use the Ridge object (part of the linear_model module) to implement ridge regression.\n",
    "\n",
    "We can specify the value of the α hyperparameter when initializing the Ridge object (the default is 1.0). However, rather than manually choosing a value, we can use cross-validation to help us choose the optimal α from a list of values.\n",
    "\n",
    "We'll discuss the specifics of cross-validation in the chapters Cross-Validation, Applying CV to Decision Trees, and Exhaustive Tuning, but for now just know that we can implement a cross-validated ridge regression using the RidgeCV object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0953359d-5afa-41f8-bf20-268fbf68f4f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients: array([0.00330919, 0.0023288 ])\n",
      "\n",
      "Intercept: 2.3379782896471344\n",
      "\n",
      "R2: 0.9758349388362844\n",
      "\n",
      "Coefficients: array([0.00330932, 0.00232767])\n",
      "\n",
      "Intercept: 2.338616852437532\n",
      "\n",
      "Chosen alpha: 0.3\n",
      "\n",
      "R2: 0.9758349386047629\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn import linear_model\n",
    "\n",
    "pizza_data = [[2100,  800],\n",
    "       [2500,  850],\n",
    "       [1800,  760],\n",
    "       [2000,  800],\n",
    "       [2300,  810]]\n",
    "pizza_prices = [10.99, 12.5 ,  9.99, 10.99, 11.99]\n",
    "\n",
    "reg = linear_model.Ridge(alpha=0.1)\n",
    "reg.fit(pizza_data, pizza_prices)\n",
    "print('Coefficients: {}\\n'.format(repr(reg.coef_)))\n",
    "print('Intercept: {}\\n'.format(reg.intercept_))\n",
    "\n",
    "r2 = reg.score(pizza_data, pizza_prices)\n",
    "print('R2: {}\\n'.format(r2))\n",
    "\n",
    "alphas = [0.1, 0.2, 0.3]\n",
    "reg = linear_model.RidgeCV(alphas=alphas)\n",
    "reg.fit(pizza_data, pizza_prices)\n",
    "print('Coefficients: {}\\n'.format(repr(reg.coef_)))\n",
    "print('Intercept: {}\\n'.format(reg.intercept_))\n",
    "print('Chosen alpha: {}\\n'.format(reg.alpha_))\n",
    "\n",
    "r2 = reg.score(pizza_data, pizza_prices)\n",
    "print('R2: {}\\n'.format(r2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2fe33d6-fb0d-4aad-af8e-c1c36be0b9bc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
