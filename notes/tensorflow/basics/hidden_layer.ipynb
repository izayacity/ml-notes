{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fb64d052-3f7e-435b-9db9-c82c10d801f6",
   "metadata": {},
   "source": [
    "## Why a single layer is limited\n",
    "In the previous chapter we saw that the single layer perceptron was unable to classify points as being inside or outside a circle centered around the origin. This was because the output of the model (the logits) only had connections directly from the input features.\n",
    "\n",
    "Why exactly is this a limitation? Think about how the connections work in a single layer neural network. The neurons in the output layer have a connection coming in from each of the neurons in the input layer, but the connection weights are all just real numbers.\n",
    "\n",
    "![title](img/single_layer_perceptron.png)\n",
    "\n",
    "The single layer neural network architecture. The weight values are denoted w1, w2, and w3.\n",
    "\n",
    "Based on the diagram, the logits can be calculated as a linear combination of the input layer and weights:\n",
    "\n",
    "logits=w\n",
    "​1\n",
    "​​ ⋅x+w\n",
    "​2\n",
    "​​ ⋅y+w\n",
    "​3\n",
    "​​ \n",
    "\n",
    "The above linear combination shows the single layer perceptron can model any linear boundary. However, the equation of the circle boundary is:\n",
    "\n",
    "x^{2} +y^{2} =r^{2}\n",
    "\n",
    "This is not an equation that can be modeled by a single linear combination.\n",
    "\n",
    "##  Hidden layers\n",
    "\n",
    "If a single linear combination doesn't work, what can we do? The answer is to add more linear combinations, as well as non-linearities. We add more linear combinations to our model by adding more layers. The single layer perceptron has only an input and output layer. We will now add an additional hidden layer between the input and output layers, officially making our model a multilayer perceptron. The hidden layer will have 5 neurons, which means that it will add an additional 5 linear combinations to the model's computation.\n",
    "\n",
    "![title](img/multilayer_perception.png)\n",
    "\n",
    "The multilayer perceptron architecture.\n",
    "\n",
    "The 5 neuron hidden layer is more than enough for our circle example. However, for very complex datasets a model could have multiple hidden layers with hundreds of neurons per layer. In the next chapter, we discuss some tips on choosing the number of neurons and hidden layers in a model.\n",
    "\n",
    "## Non-linearity\n",
    "\n",
    "We add non-linearities to our model through activation functions. These are non-linear functions that are applied within the neurons of a hidden layer. You've already seen an example of a non-linear activation function, the sigmoid function. We used this after the output layer of our model to convert the logits to probabilities.\n",
    "\n",
    "The 3 most common activation functions used in deep learning are tanh(https://en.wikipedia.org/wiki/Hyperbolic_function#Tanh), ReLU(https://en.wikipedia.org/wiki/Rectifier_(neural_networks)), and the aforementioned sigmoid)https://en.wikipedia.org/wiki/Sigmoid_function(. Each has its uses in deep learning, so it's normally best to choose activation functions based on the problem. However, the ReLU activation function has been shown to work well in most general-purpose situations, so we'll apply ReLU activation for our hidden layer.\n",
    "\n",
    "## ReLU\n",
    "\n",
    "The equation for ReLU is very simple:\n",
    "\n",
    "ReLU(x) = max(0, x)ReLU(x)=max(0,x)\n",
    "\n",
    "You might wonder why ReLU even works. While tanh and sigmoid are both inherently non-linear, the ReLU function seems pretty linear (it's just f(x) = 0 for x < 0 and f(x) = x for x ≥ 0). However, let's take a look at the following function:\n",
    "\n",
    "f(x) = ReLU(x) + ReLU(-x) + ReLU(2x - 2) + ReLU(-2x - 2)f(x)=ReLU(x)+ReLU(−x)+ReLU(2x−2)+ReLU(−2x−2)\n",
    "\n",
    "This is just a linear combination of ReLU. However, the graph it produces looks like this:\n",
    "\n",
    "![title](img/relu_graph.png)\n",
    "\n",
    "Though a bit rough on the edges, it looks somewhat like the quadratic function, f(x) = x2. In fact, with enough linear combinations and ReLU activations, a model can easily learn the quadratic transformation. This is exactly how our multilayer perceptron can learn the circle decision boundary.\n",
    "\n",
    "We've shown that ReLU is capable of being a non-linear activation function. So what makes it work well in general purpose situations? Its aforementioned simplicity. The simplicity of ReLU, specifically with respect to its gradient, allows it to avoid the vanishing gradient problem(https://en.wikipedia.org/wiki/Vanishing_gradient_problem). Furthermore, the fact that it maps all negative values to 0 actually helps the model train faster and avoid overfitting (discussed in the next chapter)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1684857c-e156-475e-9dcd-b2381cbd5c94",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_layers(inputs, output_size):\n",
    "  hidden1_inputs = inputs \n",
    "  hidden1 = tf.keras.layers.Dense(units=5,\n",
    "                            activation=tf.nn.relu,\n",
    "                            name='hidden1')(hidden1_inputs) \n",
    "                                                              \n",
    "  logits_inputs = hidden1                       \n",
    "  logits = tf.keras.layers.Dense(units=output_size,\n",
    "                           name='logits')(logits_inputs)\n",
    "  return hidden1_inputs, hidden1, logits_inputs, logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d4fdccb-b2ad-4720-8f0d-4346cb87f117",
   "metadata": {},
   "source": [
    "After adding in the hidden layer to the model, the multilayer perceptron will be able to classify the 2-D circle dataset. Specifically, the classification plot will now look like:\n",
    "\n",
    "![title](img/circle_classification.png)\n",
    "\n",
    "Blue represents points the model thinks is outside the circle and red represents points the model thinks is inside. As you can see, the model is a lot more accurate now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4e6b17f-05b2-42f2-b4fd-93a8b146ba7e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
