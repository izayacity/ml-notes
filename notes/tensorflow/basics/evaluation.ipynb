{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "247f3fee-6042-4404-8706-c95ea6f7ec44",
   "metadata": {},
   "source": [
    "## Evaluating using accuracy\n",
    "\n",
    "After training a model, it is a good idea to evaluate its performance. We do this by using a test set (i.e. data points not used in model training) and observe the model's prediction accuracy on the test set.\n",
    "\n",
    "The code for this chapter makes use of the accuracy metric defined in Chapter 4. The accuracy represents the classification accuracy of an already trained model, i.e. the proportion of correct predictions the model makes on a test set.\n",
    "\n",
    "## Different amounts of data\n",
    "\n",
    "Since we used None when defining our placeholder shapes, it allows us to run training or evaluation on any number of data points, which is super helpful since we normally want to evaluate on many more data points than the training batch size.\n",
    "\n",
    "It is good practice to split up a dataset into a three sets:\n",
    "\n",
    "Training set (~80% of dataset): Used for model training and optimization\n",
    "Validation set (~10% of dataset): Used to evaluate the model in between training runs, e.g. when tweaking model parameters like batch size\n",
    "Test set (~10% of dataset): Used to evaluate the final model, usually through some accuracy metric\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e838215c-6743-483e-a540-c327fbbd6233",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
