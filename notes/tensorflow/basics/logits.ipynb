{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3a741781-a72c-4948-a3c6-e4f9b26ded1a",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Fully-connected layer\n",
    "\n",
    "The single fully-connected layer means that the input layer, i.e. self.inputs, is directly connected to the output layer, which has output_size neurons. Each of the input_size neurons in the input layer has a connection to each neuron in the output layer, hence the fully-connected layer.\n",
    "\n",
    "![title](img/fully_connected_layer.png)\n",
    "\n",
    "Fully-connected layer with input_size = 2, output_size = 3\n",
    "\n",
    "In TensorFlow, this type of fully-connected neuron layer is implemented using tf.keras.layers.Dense, which takes in a neuron layer and output size as required arguments, and adds a fully-connected output layer with the given size to the computation graph.\n",
    "\n",
    "In addition to the input layer neurons, tf.keras.layers.Dense adds another neuron called the bias, which always has a value of 1 and has full connections to the output layer. The bias neuron helps our neural network produce better results, by allowing each fully-connected layer to model a true linear combination of the input values.\n",
    "\n",
    "![title](img/fully_connected_with_bias.png)\n",
    "\n",
    "Fully-connected layer with input_size = 2, output_size = 1, and a bias neuron.\n",
    "\n",
    "## Weighted connections\n",
    "\n",
    "The forces that drive a neural network are the real number weights attached to each connection. The weight on a connection from neuron A into neuron B tells how strongly A affects B as well as whether that effect is positive or negative, i.e. direct vs. inverse relationship.\n",
    "\n",
    "![title](img/weighted_connections.png)\n",
    "\n",
    "The diagram above has three weighted connections:\n",
    "\n",
    "A → B: Direct relationship.\n",
    "A → C: No relationship.\n",
    "A → D: Inverse relationship.\n",
    "The weighted connections allow fully-connected layers to model a linear combination of the inputs. Let's take a look at an example fully-connected layer with weights:\n",
    "\n",
    "![title](img/fully_connected_with_weights.png)\n",
    "\n",
    "Fully-connected layer with input neuron values x1 and x2, a bias neuron, and weight values w1, w2, and w3\n",
    "\n",
    "The output of this fully-connected layer is now a linear combination of the input neuron values:\n",
    "\n",
    "w\n",
    "​1\n",
    "​​ ⋅x\n",
    "​1\n",
    "​​ + w\n",
    "​2\n",
    "​​ ⋅x\n",
    "​2\n",
    "​​ + w\n",
    "​3\n",
    "​​\n",
    "\n",
    "The logits produced by our single layer perceptron are therefore just a linear combination of the input data feature values.\n",
    "\n",
    "Connection weights can be optimized through training (which we will cover in a later chapter), so that the logits produced by the neural network allow the model to make highly accurate predictions based on the input data.\n",
    "\n",
    "## Logits\n",
    "\n",
    "In classification problems they represent log-odds, which maps a probability between 0 and 1 to a real number. When output_size = 1, our model outputs a single logit per data point. The logits will then be converted to probabilities representing how likely it is for the data point to be labeled 1 (as opposed to 0).\n",
    "\n",
    "![title](img/logits_diagram.png)\n",
    "\n",
    "In the above diagram, the x-axis represents the probability and the y-axis represents the logit.\n",
    "Note the vertical asymptotes at x = 0 and x = 1.\n",
    "\n",
    "We want our neural network to produce logits with large absolute values, since those represent probabilities close to 0 (meaning we are very sure the label is 0/False) or probabilities close to 1 (meaning we are very sure the label is 1/True).\n",
    "\n",
    "\n",
    "## Regression\n",
    "\n",
    "In the next chapter, you'll be producing actual probabilities from the logits. This makes our single layer perceptron model equivalent to logistic regression. Despite the name, logistic regression is used for classification, not regression problems. If we wanted a model for regression problems (i.e. predicting a real number such as a stock price), we would have our model directly output the logits rather than convert them to probabilities. In this case, it would be better to rename logits, since they don't map to a probability anymore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08da7fdf-6539-451c-b62e-6e87e79a9e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_layers(inputs, output_size):\n",
    "  logits = tf.keras.layers.Dense(output_size,\n",
    "                           name='logits')(inputs)\n",
    "  return logits"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
