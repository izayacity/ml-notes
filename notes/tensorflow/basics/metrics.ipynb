{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2bbabfac-0fff-4c32-99ee-b8b0f7034e4a",
   "metadata": {},
   "source": [
    "## Sigmoid\n",
    "\n",
    "As discussed in the previous chapter, our model outputs logits based on the input data. These logits represent real number mappings from probabilities. Therefore, if we had the inverse mapping we could obtain the original probabilities. Luckily, we have exactly that, in the form of the sigmoid function(https://en.wikipedia.org/wiki/Sigmoid_function).\n",
    "\n",
    "![title](img/sigmoid_graph.png)\n",
    "\n",
    "The above plot shows a sigmoid function graph. The x-axis represents logits, while the y-axis represents probabilities. Note the horizontal asymptotes at y = 0 and y = 1.\n",
    "\n",
    "Using the sigmoid function (tf.math.sigmoid in TensorFlow), we can extract probabilities from the logits. In binary classification, i.e. output_size = 1, this represents the probability the model gives to the input data being labeled 1. A probability closer to 0 or 1 means the model is pretty sure in its prediction, while a probability closer to 0.5 means the model is unsure (0.5 is equivalent to a random guess of the label's value).\n",
    "\n",
    "\n",
    "## Predictions and accuracy\n",
    "\n",
    "A probability closer to 1 means the model is more sure that the label is 1, while a probability closer to 0 means the model is more sure that the label is 0. Therefore, we can obtain model predictions just by rounding each probability to the nearest integer, which would be 0 or 1. Then our prediction accuracy would be the number of correct predictions divided by the number of labels.\n",
    "\n",
    "In TensorFlow, there is a function called tf.math.reduce_mean(https://www.tensorflow.org/api_docs/python/tf/reduce_mean) which produces the overall mean of a tensor's numeric values. We can use this to calculate prediction accuracy as the mean number of correct predictions across all input data points.\n",
    "\n",
    "We use these metrics to evaluate how good our model is both during and after training. This way we can experiment with different computation graphs, such as different numbers of neurons or layers, and find which structure works best for our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a531f43-f012-45be-9721-a734655d0779",
   "metadata": {},
   "outputs": [],
   "source": [
    "probs = tf.math.sigmoid(logits)\n",
    "rounded_probs = tf.math.round(probs)\n",
    "predictions = tf.cast(rounded_probs, tf.int32)\n",
    "is_correct = tf.math.equal(predictions, labels)\n",
    "\n",
    "is_correct_float = tf.cast(is_correct, tf.float32)\n",
    "accuracy = tf.math.reduce_mean(is_correct_float)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
