{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0e345cba-a0f6-432c-a65d-f6a0e1486ec7",
   "metadata": {},
   "source": [
    "## Running the model\n",
    "\n",
    "The tf.compat.v1.Session object has an extremely important function called run. All the code written in the previous chapters was to build the computation graph of the neural network, i.e. its layers and operations. However, we can only train or evaluate the model on real input data using run. The function takes in a single required argument and a few keyword arguments.\n",
    "\n",
    "## Using run\n",
    "\n",
    "The required argument is normally either a single tensor/operation or a list/tuple of tensors and operations. Calling run on a tensor returns the value of that tensor after executing our computation graph. The output of run with a tensor input is a NumPy array.\n",
    "\n",
    "Of the keyword arguments for run, the important one for most applications is feed_dict. The feed_dict is a python dictionary. Each key is a tensor from the model's computation graph. The key's value can be a Python scalar, list, or NumPy array.\n",
    "\n",
    "We use feed_dict to pass values into certain tensors in the computation graph. In the code below, we pass in a value for inputs, which is a tf.compat.v1.placeholder object.\n",
    "\n",
    "Each tf.compat.v1.placeholder object used in the model execution must be included as a key in the feed_dict, with the corresponding value's shape and type matching the placeholder's."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "73ffe210-ecdc-42ef-a039-0fbbd7108610",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "array([1, 2, 3])\n",
      "\n",
      "(array([1, 2, 3]), 4)\n",
      "\n",
      "array([[ 1.1, -0.3],\n",
      "       [ 0.2,  0.1]], dtype=float32)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "with tf.compat.v1.Session() as sess:\n",
    "    t = tf.constant([1, 2, 3])\n",
    "    arr = sess.run(t)\n",
    "    print('{}\\n'.format(repr(arr)))\n",
    "\n",
    "    t2 = tf.constant(4)\n",
    "    tup = sess.run((t, t2))\n",
    "    print('{}\\n'.format(repr(tup)))\n",
    "    \n",
    "    inputs = tf.compat.v1.placeholder(tf.float32, shape=(None, 2))\n",
    "    feed_dict = {\n",
    "        inputs: [[1.1, -0.3], [0.2, 0.1]]\n",
    "    }\n",
    "    arr = sess.run(inputs, feed_dict=feed_dict)\n",
    "    print('{}\\n'.format(repr(arr)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dc3ba78-cea2-46e1-a29f-9ded298380ca",
   "metadata": {},
   "source": [
    "## Initializing variables\n",
    "\n",
    "When we call run, every tensor in the model's computation graph must either already have a value or must be fed in a value through feed_dict. However, when we start training from scratch, none of our variables (e.g. weights) have values yet. We need to initialize all the variables using tf.compat.v1.global_variables_initializer. This returns an operation that, when used as the required argument in run, initializes all the variables in the model.\n",
    "\n",
    "In the code below, the variables that are initialized are part of tf.keras.layers.Dense. The variable initialization process is defined internally by the function. In this case, the variables are initialized in a way that results in zero logits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f9e47326-a299-4eb2-8de5-f7c532979eae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "array([[-0.0709058 ],\n",
      "       [-0.06924351]], dtype=float32)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with tf.compat.v1.Session() as sess:\n",
    "  inputs = tf.compat.v1.placeholder(tf.float32, shape=(None, 2))\n",
    "  feed_dict = {\n",
    "    inputs: [[1.1, -0.3],\n",
    "            [0.2, 0.1]]\n",
    "  }\n",
    "  logits = tf.keras.layers.Dense(units=1, name='logits')(inputs)\n",
    "  init_op = tf.compat.v1.global_variables_initializer()\n",
    "  sess.run(init_op) # variable initialization\n",
    "  arr = sess.run(logits, feed_dict=feed_dict)\n",
    "  print('{}\\n'.format(repr(arr)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78244028-3be3-4ff0-85ad-bcd341e7e0f0",
   "metadata": {},
   "source": [
    "## Training logistics\n",
    "\n",
    "The num_steps argument represents the number of iterations we use to train our model. Each iteration we train the model on a batch of data points. So input_data is essentially a large dataset divided into chunks (i.e. batches), and each iteration we train on a specific batch of points and their corresponding labels.\n",
    "\n",
    "The batch size determines how the model trains. Larger batch sizes usually result in faster training but less accurate models, and vice-versa for smaller batch sizes. Choosing the batch size is a speed-precision tradeoff.\n",
    "\n",
    "When training a neural network, it's usually a good idea to print out the loss every so often, so you know that the model is training correctly and to stop the training when the loss has converged to a minimum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f84522f6-8b4d-44ad-a698-200f7fd35cde",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
