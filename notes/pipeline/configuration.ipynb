{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "27184f20-225e-4683-b9f1-e485b6005e81",
   "metadata": {},
   "source": [
    "## Shuffling\n",
    "\n",
    "When using a dataset to train a machine learning model, there are certain things we need to do to properly configure the dataset. When we first create a dataset from NumPy arrays or files, the observations may be ordered in a particular way. For example, many data files will sort the data observations by some particular feature, like a person’s name or year.\n",
    "\n",
    "While systematic ordering of data files makes it easier for humans to look over the data, it actually hinders the training of a machine learning model. The model will learn to make predictions based on the ordering of the observations rather than the observations themselves, which is not what we want our model to do. To avoid this, we need to randomly shuffle our dataset prior to training a model, which is done with the shuffle function(https://www.tensorflow.org/api_docs/python/tf/data/Dataset#shuffle).\n",
    "\n",
    "shuffle(\n",
    "    buffer_size, seed=None, reshuffle_each_iteration=None, name=None\n",
    ")\n",
    "\n",
    "This dataset fills a buffer with buffer_size elements, then randomly samples elements from this buffer, replacing the selected elements with new elements. For perfect shuffling, a buffer size greater than or equal to the full size of the dataset is required.\n",
    "\n",
    "For instance, if your dataset contains 10,000 elements but buffer_size is set to 1,000, then shuffle will initially select a random element from only the first 1,000 elements in the buffer. Once an element is selected, its space in the buffer is replaced by the next (i.e. 1,001-st) element, maintaining the 1,000 element buffer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0a69efb6-2286-47e6-bd78-00b3e1ab6077",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<ShuffleDataset element_spec=TensorSpec(shape=(5,), dtype=tf.float64, name=None)>\n",
      "<ShuffleDataset element_spec=TensorSpec(shape=(5,), dtype=tf.float64, name=None)>\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "data = np.random.uniform(-100, 100, (1000, 5))\n",
    "original = tf.compat.v1.data.Dataset.from_tensor_slices(data)\n",
    "\n",
    "shuffled1 = original.shuffle(100)\n",
    "print(shuffled1)\n",
    "\n",
    "shuffled2 = original.shuffle(len(data))\n",
    "print(shuffled2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9af6b47e-6e7e-4aff-ad1d-75bfdf566b63",
   "metadata": {},
   "source": [
    "The buffer size (specified by the required argument of shuffle) essentially dictates how random our shuffling is. The larger the buffer size, the more uniformly random the shuffling will be.\n",
    "\n",
    "If the buffer size is 1, then no shuffling occurs. If the buffer size is \\ge≥ the number of total observations (the case for shuffled2), then the shuffling is uniformly random across the entire dataset. When the buffer size is somewhere in between (the case for shuffled1), shuffling will still occur but it will not be uniformly random for all the observations.\n",
    "\n",
    "In most cases, uniform shuffling is the best option, since that’s the safest bet to avoiding any systematic ordering of the initial data observations. However, for datasets that take up a lot of memory, it may be useful to initially shuffle the entire dataset once, then use a smaller buffer size for faster training.\n",
    "\n",
    "## Epochs\n",
    "\n",
    "Aside from randomly shuffling, we also need to configure a dataset so that training can be done for multiple epochs. An epoch refers to a single training run over the entire dataset. We normally need to go through several epochs of a dataset before the model is finished training.\n",
    "\n",
    "Without any configuration, a dataset can only be used to train a model for one epoch. If we try to train for more than one epoch, we get back an OutOfRangeError. To fix this, we use the repeat function to specify the number of epochs we can run a dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c6491381-1631-4d3d-b39c-955aabf3a648",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<RepeatDataset element_spec=TensorSpec(shape=(5,), dtype=tf.float64, name=None)>\n",
      "<RepeatDataset element_spec=TensorSpec(shape=(5,), dtype=tf.float64, name=None)>\n",
      "<RepeatDataset element_spec=TensorSpec(shape=(5,), dtype=tf.float64, name=None)>\n"
     ]
    }
   ],
   "source": [
    "data = np.random.uniform(-100, 100, (1000, 5))\n",
    "original = tf.compat.v1.data.Dataset.from_tensor_slices(data)\n",
    "\n",
    "repeat1 = original.repeat(1)\n",
    "print(repeat1)\n",
    "\n",
    "repeat2 = original.repeat(100)\n",
    "print(repeat2)\n",
    "\n",
    "repeat3 = original.repeat()\n",
    "print(repeat3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01699184-8d4a-47a5-9c71-19c259c4f326",
   "metadata": {},
   "source": [
    "The repeat function takes in a single (optional) argument, which specifies how many epochs we can run the dataset for.\n",
    "\n",
    "The repeat1 dataset is explicitly configured for one epoch, while repeat2 is configured for 100 epochs. This means we can use repeat1 for one full run through of original and repeat2 for 100 full run throughs of original, before getting back an OutOfRangeError.\n",
    "\n",
    "For repeat3, we didn’t pass in any argument. This is the same as passing in None, which is the default value for the optional argument. It means that repeat3 can be run indefinitely, and will never raise an OutOfRangeError.\n",
    "\n",
    "## Batching\n",
    "\n",
    "The default number of data observations we use for each dataset iteration is one. This can be incredibly slow when training or evaluating very large datasets, which can have millions of observations. Luckily, we can explicitly set the number of observations per iteration (i.e. the batch size) using the batch function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "08f0fffe-bac8-4496-8732-5b89dd55d5fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<BatchDataset element_spec=TensorSpec(shape=(None, 5), dtype=tf.float64, name=None)>\n",
      "<BatchDataset element_spec=TensorSpec(shape=(None, 5), dtype=tf.float64, name=None)>\n"
     ]
    }
   ],
   "source": [
    "data = np.random.uniform(-100, 100, (1000, 5))\n",
    "original = tf.compat.v1.data.Dataset.from_tensor_slices(data)\n",
    "\n",
    "batch1 = original.batch(1)\n",
    "print(batch1)\n",
    "\n",
    "batch2 = original.batch(100)\n",
    "print(batch2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8bb26ff-0ce9-4487-9130-b9698807ce16",
   "metadata": {},
   "source": [
    "The batch function takes in a required argument, representing the batch size per iteration. In the example, batch1 will use one data observation per iteration while batch2 will use 100. So at each iteration, the shape of the data for batch1 will be (1, 5), while the shape of the data for batch2 will be (100, 5).\n",
    "\n",
    "In general, it is a good idea to explicitly set the batch size using batch, even if the batch size is just 1. Without setting the batch size, the shape of the data at each iteration is 1-D (in the example, it is (5,)). This can potentially cause issues, since a single observation might be interpreted as a batch (e.g. a shape of (5,) could be viewed as a batch of 5 observations).\n",
    "\n",
    "The optimal batch size will differ depending on the size of the dataset and the problem itself. The batch size can also change at different points of the training. Initially, you may want a larger batch size so the model can train faster in its early stages, then gradually decrease the batch size as the model gets closer to converging so as to not overshoot the convergence point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c568719e-1d9d-44d9-9124-32cc3a5fe3b6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
