{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "49ac3464-7dd0-410c-89cf-87a3f8f0d7db",
   "metadata": {},
   "source": [
    "## Input pipeline\n",
    "\n",
    "In TensorFlow, the input pipeline for executing a machine learning model is represented by the Dataset class (which weâ€™ll refer to as simply a dataset). A dataset can be created for a variety of input values, from NumPy arrays to protocol buffers. The most basic way to create a dataset is with the tf.data.Dataset.from_tensor_slices function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "26c6a8a0-43aa-486b-bd47-a487c71219b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<TensorSliceDataset element_spec=TensorSpec(shape=(2,), dtype=tf.float64, name=None)>\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "data = np.array([[  1. ,   2.1],\n",
    "       [  2. ,   3. ],\n",
    "       [  8.1, -10. ]])\n",
    "\n",
    "d1 = tf.data.Dataset.from_tensor_slices(data)\n",
    "\n",
    "print(d1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08b21f7a-77c4-4d0f-acd0-2a63dc0b868f",
   "metadata": {},
   "source": [
    "In the example, d1 is a dataset containing the data from data. The dataset consists of three observations, with each observation being a row in data. Since each row of data has two columns, the observations in d1 have shape (2,).\n",
    "\n",
    "We can also create datasets from tuple inputs. This is useful when we want to create a dataset from both feature data and labels for each data observation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "60917694-55b4-4983-bb16-500008c80d73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<TensorSliceDataset element_spec=(TensorSpec(shape=(3,), dtype=tf.float64, name=None), TensorSpec(shape=(), dtype=tf.int32, name=None))>\n"
     ]
    }
   ],
   "source": [
    "data = np.array([[1. , 2. , 3. ],\n",
    "       [1.1, 0. , 8. ]])\n",
    "\n",
    "labels = np.array([1, 0])\n",
    "\n",
    "d2 = tf.data.Dataset.from_tensor_slices((data, labels))\n",
    "\n",
    "print(d2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "858c4c2d-edee-41b8-b300-89e5d3c1a837",
   "metadata": {},
   "source": [
    "In the example, d2 is a dataset containing the data from data and the observation labels from labels. There are two total observations, and each observation has shape (3,), since data has three columns.\n",
    "\n",
    "## Image file dataset\n",
    "\n",
    "The from_tensor_slices function is not limited to just taking NumPy arrays as input. For example, we can use it to create a dataset of file names. A popular application of this is creating a dataset for image files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "33880c78-879e-465b-92c8-fd5c9628a103",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<TensorSliceDataset element_spec=TensorSpec(shape=(), dtype=tf.string, name=None)>\n",
      "<TensorSliceDataset element_spec=(TensorSpec(shape=(), dtype=tf.string, name=None), TensorSpec(shape=(), dtype=tf.int32, name=None))>\n"
     ]
    }
   ],
   "source": [
    "filenames = ['img1.jpg', 'img2.jpg']\n",
    "img_d1 = tf.data.Dataset.from_tensor_slices(filenames)\n",
    "print(img_d1)\n",
    "\n",
    "labels = np.array([1, 0])\n",
    "img_d2 = tf.data.Dataset.from_tensor_slices((filenames, labels))\n",
    "print(img_d2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c24e595-dbfd-471d-a708-69ca586cf038",
   "metadata": {},
   "source": [
    "## Specialized datasets\n",
    "\n",
    "Apart from the from_tensor_slices function, we can also use TFRecordDataset and TextLineDataset to create specialized datasets for protocol buffers and text data, respectively.\n",
    "\n",
    "The TFRecordDataset takes in a list of TFRecords files and creates a dataset where each observation is an individual serialized protocol buffer. In the example, d1 contains the serialized protocol buffers from 'one.tfrecords' and 'two.tfrecords'.\n",
    "\n",
    "The TextLineDataset takes in a list of text files and creates a dataset where each observation is a separate line from the text files. In the example, d2 contains the lines from 'lines.txt'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7da85388-c634-4034-974e-65c277cac544",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<TFRecordDatasetV2 element_spec=TensorSpec(shape=(), dtype=tf.string, name=None)>\n",
      "<TextLineDatasetV2 element_spec=TensorSpec(shape=(), dtype=tf.string, name=None)>\n"
     ]
    }
   ],
   "source": [
    "records_files = ['one.tfrecords', 'two.tfrecords']\n",
    "d1 = tf.data.TFRecordDataset(records_files)\n",
    "print(d1)\n",
    "\n",
    "txt_files = ['lines.txt']\n",
    "d2 = tf.data.TextLineDataset(txt_files)\n",
    "print(d2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1750faf-8643-4061-bac5-cb02467b33f2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
