{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8c01f9d6-a31f-4edf-8ff6-7f6b80817977",
   "metadata": {},
   "source": [
    "## Input pipeline\n",
    "\n",
    "The process in which data is loaded from files and fed into a machine learning model is known as the input pipeline. Since the input pipeline handles a large amount of data for machine learning projects, we need it to be as efficient as possible.\n",
    "\n",
    "![title](img/input_pipeline.png)\n",
    "\n",
    "A flexible and efficient format for storing large amounts of data is Google’s protocol buffer. The protocol buffer is similar to JSON and XML (another feature-based data format), but uses less space and is faster to process. When used with TensorFlow, protocol buffers make the input pipeline for large datasets much more streamlined.\n",
    "\n",
    "## TensorFlow protocol buffer\n",
    "\n",
    "Since protocol buffers use a structured format when storing data, they can be represented with Python classes. In TensorFlow, the tf.train.Example class represents the protocol buffer used to store data for the input pipeline.\n",
    "\n",
    "Each individual tf.train.Example object describes data for a single dataset observation (e.g. a single row in a data table). We convert raw data to a protocol buffer by initializing a tf.train.Example object with the data’s values.\n",
    "\n",
    "When we initialize a tf.train.Example object, we need to set that object’s features argument to a tf.train.Features object. The tf.train.Features class is initialized by setting the feature field to a dictionary that maps feature names to feature values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "591a0d1e-a64e-4a61-98c2-aef821e1a200",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Parameter to MergeFrom() must be instance of same class: expected tensorflow.Feature got tensorflow.Int64List.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [4]\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m weight \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mtrain\u001b[38;5;241m.\u001b[39mFloatList(value\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m88.19999694824219\u001b[39m])\n\u001b[0;32m      6\u001b[0m f_dict \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mage\u001b[39m\u001b[38;5;124m'\u001b[39m: age,\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mweight\u001b[39m\u001b[38;5;124m'\u001b[39m: weight\n\u001b[0;32m      9\u001b[0m }\n\u001b[1;32m---> 10\u001b[0m features \u001b[38;5;241m=\u001b[39m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mFeatures\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mf_dict\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# f_dict is a dict\u001b[39;00m\n\u001b[0;32m     11\u001b[0m ex \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mtrain\u001b[38;5;241m.\u001b[39mExample(features\u001b[38;5;241m=\u001b[39mfeatures)\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mrepr\u001b[39m(ex))\n",
      "\u001b[1;31mTypeError\u001b[0m: Parameter to MergeFrom() must be instance of same class: expected tensorflow.Feature got tensorflow.Int64List."
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "age = tf.train.Int64List(value=[12])\n",
    "weight = tf.train.FloatList(value=[88.19999694824219])\n",
    "\n",
    "f_dict = {\n",
    "    'age': age,\n",
    "    'weight': weight\n",
    "}\n",
    "features = tf.train.Features(feature=f_dict)  # f_dict is a dict\n",
    "ex = tf.train.Example(features=features)\n",
    "print(repr(ex))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3d797c1-06d2-4568-ba89-0fd52e487c3d",
   "metadata": {},
   "source": [
    "## Bytes and text\n",
    "\n",
    "When dealing with datasets containing bytes (e.g. images or videos) or text (e.g. articles or sentences), it is beneficial to first read all the data files and then store the read data in the bytes_list field of a tf.train.Feature. This saves us from having to open each individual file within our input pipeline, which can drastically improve efficiency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "04e97e22-4f9c-44c6-a1ff-7cefdf29e326",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'text/story.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [17]\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtext/story.txt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m      4\u001b[0m     words \u001b[38;5;241m=\u001b[39m f\u001b[38;5;241m.\u001b[39mread()\u001b[38;5;241m.\u001b[39msplit()\n\u001b[0;32m      6\u001b[0m encw \u001b[38;5;241m=\u001b[39m [w\u001b[38;5;241m.\u001b[39mencode() \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m words]\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'text/story.txt'"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "with open('text/story.txt') as f:\n",
    "    words = f.read().split()\n",
    "\n",
    "encw = [w.encode() for w in words]\n",
    "words_feature = tf.train.Feature(\n",
    "    bytes_list=tf.train.BytesList(value=encw))\n",
    "print(repr(words_feature))\n",
    "\n",
    "with open('img/input_pipeline.PNG', 'rb') as f:\n",
    "    img_bytes = f.read()\n",
    "\n",
    "img_feature = tf.train.Feature(\n",
    "    bytes_list=tf.train.BytesList(value=[img_bytes]))\n",
    "print(repr(img_feature))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "832d407e-ce9a-4855-ba19-26d2d6244689",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
